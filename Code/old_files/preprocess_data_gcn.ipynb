{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GCN, we need to create an adjacency matrix of the graph. As the size of the graph is massive (1.2 million paper nodes), we need to use a sparse matrix due to its efficiency. It is also a requirement for training the model.\n",
    "\n",
    "First, let's practice with just the citation graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors = np.load(\"test_data/author_array_test.npy\")\n",
    "path = os.getcwd()\n",
    "size = \"small\"\n",
    "edgelist = np.load(\"{}/test_data_collection/test_data_{}/cites_{}.npy\".format(path,size,size))\n",
    "labels = np.load(\"{}/test_data_collection/test_data_{}/labels_{}.npy\".format(path,size,size))\n",
    "papers = np.load(\"{}/test_data_collection/test_data_{}/papers_{}.npy\".format(path,size,size))\n",
    "#author_to_paper = np.load(\"test_data/edgelist_writes_test.npy\")\n",
    "\n",
    "# Use this to fix the difference\n",
    "#np.setdiff1d(papers, np.unique(paper_to_paper))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    \n",
    "ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "\n",
    "ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "\n",
    "ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edgelist_reindexed = []\n",
    "papers_reindexed = []\n",
    "mapping = {}\n",
    "\n",
    "# reindex edges and papers to start from 0\n",
    "for k in range(len(papers)):\n",
    "    mapping[papers[k]] = k\n",
    "    papers_reindexed.append(k)\n",
    "papers = np.array(papers_reindexed)\n",
    "\n",
    "for m in range(len(edgelist.T)):\n",
    "    edge = edgelist[:,m]\n",
    "    edgelist_reindexed.append([mapping[edge[0]],mapping[edge[1]]])\n",
    "edgelist = np.array(edgelist_reindexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0\n",
      "train 4\n",
      "train 9\n",
      "train 17\n",
      "train 23\n",
      "train 30\n",
      "train 37\n",
      "train 40\n",
      "train 67\n",
      "train 69\n",
      "train 71\n",
      "train 78\n",
      "train 99\n",
      "train 101\n",
      "train 104\n",
      "train 110\n",
      "train 122\n",
      "train 123\n",
      "train 126\n",
      "train 128\n",
      "train 130\n",
      "train 134\n",
      "train 138\n",
      "train 159\n",
      "train 169\n",
      "train 172\n",
      "train 180\n",
      "train 183\n",
      "train 187\n",
      "train 193\n",
      "train 198\n",
      "train 227\n",
      "train 261\n",
      "train 279\n",
      "train 282\n",
      "train 296\n",
      "train 297\n",
      "train 298\n",
      "train 301\n",
      "train 311\n",
      "train 313\n",
      "train 315\n",
      "train 322\n",
      "train 359\n",
      "train 362\n",
      "train 369\n",
      "train 378\n",
      "train 381\n",
      "train 382\n",
      "train 383\n",
      "train 386\n",
      "train 402\n",
      "train 404\n",
      "train 405\n",
      "train 411\n",
      "train 417\n",
      "train 418\n",
      "train 422\n",
      "train 431\n",
      "train 435\n",
      "train 444\n",
      "train 450\n",
      "train 455\n",
      "train 457\n",
      "train 458\n",
      "train 468\n",
      "train 474\n",
      "train 475\n",
      "train 478\n",
      "train 481\n",
      "train 488\n",
      "train 500\n",
      "train 501\n",
      "train 505\n",
      "train 509\n",
      "train 512\n",
      "train 519\n",
      "train 527\n",
      "train 534\n",
      "train 535\n",
      "train 540\n",
      "train 554\n",
      "train 558\n",
      "train 575\n",
      "train 583\n",
      "train 599\n",
      "train 603\n",
      "train 607\n",
      "train 611\n",
      "train 612\n",
      "train 613\n",
      "train 629\n",
      "train 633\n",
      "train 634\n",
      "train 640\n",
      "train 652\n",
      "train 654\n",
      "train 655\n",
      "train 663\n",
      "train 665\n",
      "train 682\n",
      "train 684\n",
      "train 694\n",
      "train 701\n",
      "train 705\n",
      "train 708\n",
      "train 715\n",
      "train 719\n",
      "train 731\n",
      "train 734\n",
      "train 736\n",
      "train 737\n",
      "train 742\n",
      "train 751\n",
      "train 752\n",
      "train 777\n",
      "train 782\n",
      "train 784\n",
      "train 792\n",
      "train 793\n",
      "train 804\n",
      "train 816\n",
      "train 825\n",
      "train 845\n",
      "train 850\n",
      "train 855\n",
      "train 859\n",
      "train 860\n",
      "train 869\n",
      "train 877\n",
      "train 880\n",
      "train 884\n",
      "train 889\n",
      "train 890\n",
      "train 891\n",
      "train 915\n",
      "train 923\n",
      "train 942\n",
      "train 944\n",
      "train 945\n",
      "train 948\n",
      "train 951\n",
      "train 957\n",
      "train 973\n",
      "train 975\n",
      "train 981\n",
      "train 986\n",
      "train 988\n",
      "train 996\n",
      "train 999\n",
      "train 1004\n",
      "train 1008\n",
      "train 1016\n",
      "train 1021\n",
      "train 1022\n",
      "train 1033\n",
      "train 1045\n",
      "train 1054\n",
      "train 1055\n",
      "train 1059\n",
      "train 1064\n",
      "train 1068\n",
      "train 1071\n",
      "train 1080\n",
      "train 1085\n",
      "train 1086\n",
      "train 1093\n",
      "train 1101\n",
      "train 1132\n",
      "train 1141\n",
      "train 1145\n",
      "train 1155\n",
      "train 1163\n",
      "train 1174\n",
      "train 1178\n",
      "train 1187\n",
      "train 1189\n",
      "train 1198\n",
      "train 1204\n",
      "train 1210\n",
      "train 1213\n",
      "train 1223\n",
      "train 1225\n",
      "train 1235\n",
      "train 1240\n",
      "train 1253\n",
      "test 1268\n",
      "test 1274\n",
      "test 1278\n",
      "test 1283\n",
      "test 1286\n",
      "test 1299\n",
      "test 1325\n",
      "test 1327\n",
      "test 1330\n",
      "test 1335\n",
      "test 1338\n",
      "test 1343\n",
      "test 1346\n",
      "test 1347\n",
      "test 1359\n",
      "test 1360\n",
      "test 1374\n",
      "test 1380\n",
      "test 1382\n",
      "test 1383\n",
      "test 1385\n",
      "test 1388\n",
      "test 1390\n",
      "test 1407\n",
      "test 1412\n",
      "test 1434\n",
      "test 1436\n",
      "test 1439\n",
      "test 1447\n",
      "test 1452\n",
      "test 1455\n",
      "test 1457\n",
      "test 1470\n",
      "test 1477\n",
      "test 1487\n",
      "test 1492\n",
      "test 1507\n",
      "test 1509\n",
      "test 1512\n",
      "test 1514\n",
      "test 1522\n",
      "test 1525\n",
      "test 1531\n",
      "test 1554\n",
      "test 1568\n",
      "test 1570\n",
      "test 1571\n"
     ]
    }
   ],
   "source": [
    "labels = np.load(\"{}/test_data_collection/test_data_{}/labels_{}.npy\".format(path,size,size)) # 153 classes in the data\n",
    "known_labels = np.where(labels!=-1)[0]\n",
    "train_test_index_split= int(0.8*len(known_labels))\n",
    "train_index = []\n",
    "test_index = []\n",
    "j = 0\n",
    "label_matrix = np.zeros((len(labels),153))\n",
    "for i in range(len(labels)):\n",
    "    if not labels[i] == -1:\n",
    "        label_matrix[i,int(labels[i])] = 1\n",
    "        if j < train_test_index_split:\n",
    "            train_index.append(i)\n",
    "            j += 1\n",
    "        else:\n",
    "            test_index.append(i)\n",
    "\n",
    "train_labels = label_matrix[train_index,:]\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.y', 'wb') as handle:\n",
    "    pickle.dump(train_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "test_labels = label_matrix[test_index,:]\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.ty', 'wb') as handle:\n",
    "    pickle.dump(test_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "label_matrix_no_test = np.delete(label_matrix, test_index, axis=0) # remove test index\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.ally', 'wb') as handle:\n",
    "    pickle.dump(label_matrix_no_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.test.index', 'w') as f:\n",
    "    for index in test_index:\n",
    "        f.write(str(index) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "\n",
    "ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "\n",
    "ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = np.load(\"{}/test_data_collection/test_data_{}/features_{}.npy\".format(path,size,size))\n",
    "feature_matrix_no_test = np.delete(feature_matrix, test_index, axis=0) # remove test index\n",
    "feature_matrix_train = feature_matrix[train_index]\n",
    "feature_matrix_test = feature_matrix[test_index]\n",
    "\n",
    "sparse_feature_matrix = csr_matrix(feature_matrix_no_test)\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.allx', 'wb') as handle:\n",
    "    pickle.dump(sparse_feature_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "sparse_feature_matrix_train = csr_matrix(feature_matrix_train)\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.x', 'wb') as handle:\n",
    "    pickle.dump(sparse_feature_matrix_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "sparse_feature_matrix_test = csr_matrix(feature_matrix_test)\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.tx', 'wb') as handle:\n",
    "    pickle.dump(sparse_feature_matrix_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict object;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = dict()\n",
    "for edge in edgelist:\n",
    "    if edge[0] in G:\n",
    "        G[edge[0]].append(int(edge[1]))\n",
    "    else:\n",
    "        G[edge[0]] = [int(edge[1])]\n",
    "\n",
    "    if edge[1] in G: \n",
    "        G[edge[1]].append(int(edge[0]))\n",
    "    else:\n",
    "        G[edge[1]] = [int(edge[0])]\n",
    "\n",
    "for no_edge in np.setdiff1d(papers_reindexed, np.unique(edgelist)):\n",
    "    G[no_edge] = [int(no_edge)]\n",
    "\n",
    "with open('/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.graph', 'wb') as handle:\n",
    "    pickle.dump(G, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1535, 768)\n",
      "(1535, 153)\n",
      "1582\n",
      "46\n",
      "(47, 768)\n",
      "(47, 153)\n",
      "(186, 768)\n",
      "(186, 153)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allx = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.allx')\n",
    "ally = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.ally')\n",
    "graph = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.graph')\n",
    "index = pd.read_csv(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.test.index')\n",
    "tx = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.tx')\n",
    "ty = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.ty')\n",
    "x = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.x')\n",
    "y = pd.read_pickle(r'/Users/Samu/Desktop/Koulu/Gradu/gcn/gcn/data/ind.mag_prac.y')\n",
    "\n",
    "print(allx.shape)\n",
    "print(ally.shape)\n",
    "print(len(graph))\n",
    "print(len(index))\n",
    "print(tx.shape)\n",
    "print(ty.shape)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
